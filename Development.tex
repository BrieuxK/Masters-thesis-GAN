\section{Network development history}

The original code which inspired this work can be found at [\ref{github GAN}]. It is originally designed to be trained on the MNIST dataset and thus, produce sample of handwritten digits. In this kind of GANs, some parts are precisely dedicated to image processing, as convolutional layers or max pooling for example. These will be no use for this work, thus those are not implemented in our GAN. Moreover, we want to plot the evolution of loss function for both the generator and discriminator to monitor the learning process as well as plotting the learning rate over time. The latter plots will allow us to double check whether or not our adaptive learning rate techniques work as intended.\\

The whole process of fine-tuning hyperparameters and stabilizing the whole network represents a major task of this thesis. The estimated required time to achieve this is approximately ?? weeks.

\subsection{First results}

Right after the basic modifications to transition from image-related to data-related, here are the distributions generated by two different networks, with the same set of hyperparameters. The first one is characterized by a single hidden layer with 8 nodes, while the second one is made of 4 hidden layers with 256 nodes each.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.45]{invMass_generated_simple_network.png}
        \label{complexe}
    \end{subfigure}
    \hspace{1.3cm}
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.45]{invMass_generated.png}
        \label{simple}
    \end{subfigure}
    \caption{Example of output for different networks. Left : simple network. Right : complex network.}
\end{figure}

\subsection{First convergences}

Once the architecture of the model set, we can try different set of hyperparameters. At first glance, these results look satisfying. However, due to several reasons, as a (very) reduced number of epochs for the training, this network is very unstable. As stated in a previous section, totally different output distributions can be generated for the exact same set of hyperparameters. Thus, the fine-tuning of hyperparamters is near impossible since no consistent benchmark exists.
This could be explained by the loss function being stuck in a local minimum, hence the different results obtained. Indeed, at this point, there is no weight initializer, no adaptive learning rate techniques, ... to address this issue.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.37]{Development/5x16_0.00001_3000.png}
    \end{subfigure}
    \hspace{1.3cm}
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.47]{Development/5x16_0.001_3000.png}
    \end{subfigure}
    \caption{Example of output for a same network but with slighlty different learning rate}
\end{figure}

\subsection{Number of epochs and loss functions}

The first idea to get out of these local minima was to significantly expand ($\times 10-15$) the training of the network. Moreover, we plot the evolution of the loss function to gain a better understanding of the process.\\
Unfortunately, it modifies the shape of the output distribution, worsening the overall result. This is a terrible sign, increasing the training shouldn't drive the network away from the expected distribution, it should do the complete opposite. Sometimes even causing overfitting. In addition, the loss function can adopt unexpected behaviour, reinforcing our bad resentment.
Indeed, the decreases of both losses shown in Fig.(\ref{distrib + lr x2}c) and in Fig.(\ref{distrib + lr x2}d) are clearly different. It could be explained by the gradient being stuck in a local minimum or a saddle point in the case of Fig.(\ref{distrib + lr x2}d). It means that, unfortunately, increasing the number of epochs is not the solution to escape local minima and saddle points in our case, we need to find something else.
However, it doesn't mean the change has to be reverted. It might be a step in the right direction, however, there is still quite a run to go.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.45]{Development/bad_distrib_convergence_v1.png}
    \end{subfigure}
    \hspace{1.3cm}
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.45]{Development/decent_distrib_stagnation_v2.png}
    \end{subfigure}
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.45]{Development/convergent_shaky_slope.png}
    \end{subfigure}
    \hspace{1.15cm}
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.45]{straight_slope_CE.png}
    \end{subfigure}
    \caption{Example of output for a same divergent network with the corresponding loss function (binary cross-entropy) evolution.}
    \label{distrib + lr x2}
\end{figure}

\subsection{Encouraging follow-up ?}
To address this problem of inconsistency, adaptive learning rate methods (such as cyclic LR, LR scheduler or reduce LR on plateau) were considered. Here are the generated distributions : Fig.(\ref{follow up}). It does not look obvious, but with these improvements the network is now stable. The fine-tuning is then much easier to perform.\\
However, the network remains far from the expected distribution.\\
From now on, the loss function always adopt a similar shape to the lower-left plot at Fig.(\ref{distrib + lr x2}).

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.45]{Development/5x32_CyclicLR_30k.png}
    \end{subfigure}
    \hspace{1.3cm}
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[scale = 0.45]{distrib_with_scheduler.png}
    \end{subfigure}
    \caption{Implementation of adaptive LR methods. Left : Cyclic (triangular). Right : Scheduler.}
    \label{follow up}
\end{figure}

\subsection{Encouraging follow-up !}

After the implementation of \textit{He} weight initializer, of batch normalization with a greater batch size coupled to another cyclic learning rate strategy (decreasing triangular), here is the output of the GAN.
Despite a clear improvement on the previous case, the generated distribution remains at a distance from the input, mainly because the generated simulation is shifted to the right in comparison to the inputs.

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.45]{Development/10_04_best_result_so_far.png}
    \caption{Output provided by an almost converging network}
    \label{fig:enter-label}
\end{figure}

\subsection{Final result for 1DGAN}

Once the shifting problem solved using more adapted rescaling techniques, we obtain the following result for the 1DGAN :

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.45]{Development/best_result_for_MuonPt.png}
    \caption{Best result obtained so far.}
    \label{fig:enter-label}
\end{figure}

%So far, it has been fairly easy to compare the generated with the input distributions using only histograms. However, these two samples being very similar, visual comparison is too weak to base our analysis on. From now on, the \textit{Kolmogorov-Smirnov test} [\ref{KS test}] and the \textit{Z-test} [\ref{Z test}] will also be used to determine the similarity between the two distributions we are interested in.

\subsection{2-dimensional GAN}

As one can see on Fig.(\ref{2dGAN}), the network converges in a satisfying manner to the input distributions. Although, the generated populations show difficulty to match peaks of the initial sample.

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.37]{Development/18_04_best_so_far.png}
    \caption{Output provided by a 2D GAN. The variables generated are the tranverse momentum of muons and the invariant mass of the system}
    \label{2dGAN}
\end{figure}

One-dimensional histograms are good tools to check how close to the initial distribution our generated sample stands. However, the main goal of our network is not only to replicate the inputs, but also to mimic the correlations between the variables set as input.\\
To visualize these dependencies, we use two-dimensional histograms as shown in Fig.(\ref{hist2D}). On the first plot, the correlation between the variable "invMass" and "MuonsPt" are shown for the generated data, while for the following plot, it is the correlation between the same variables but for input data. According to these two previous plots, I can assume that the network is doing a satisfying job in reproducing correlations.\\


\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.42]{MuonPt_corr.png}
    \caption{Correlations between the two variables. Two zones stand out, the biggest corresponds to Drell-Yan events where a $Z$ is the mediator boson, while the other stands for a $\gamma$ as mediator boson.}
    \label{hist2D}
\end{figure}

\subsection{3-dimensional GAN}

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.45]{Development/19_04_3D_GAN.png}
    \caption{Output provided by a 3D GAN. The variables generated are the tranverse momentum of muons, the invariant mass of the system and the missing transverse momentum.}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.4]{Development/3D_Hist2D_vanilla.png}
    \caption{Correlations between the two variables generated by the network.}
    \label{fig:enter-label}
\end{figure}

I decide to use the mutual information of two variables to compute the dependency between them. The result obtained is : 

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Mutual Information score} & \textbf{Outputs} & \textbf{Inputs} \\
        \hline
        MI(MuonPt, invMass) & 9.3875 & 8.6731 \\
        MI(invMass, MET\_pt) & 9.3880 & 8.6733 \\
        MI(MuonPt, MET\_pt) & 9.3908 & 8.6731 \\
        \hline
    \end{tabular}
    \caption{Mutual Information Values for a 3-D GAN.}
    \label{tab:mi_values}
\end{table}

\newpage

\subsection{Conditional GAN}

The next step is to transition to a conditional GAN, which is the goal of this work. The idea is simply to provide a new piece of information to the generator, called a "label". This label refers to the type of data that the network will be generating. We select the presence of b-tagged jets as the label, thus creating two classes : without b-jets and with at least one b-jet. It is also the variable blinded in the training sample.\\

First, let's check how is the network performing on each variables.\\

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.45]{Development/19_04_3D_GAN.png}
    \caption{PLACEHOLDER}
    \label{fig:enter-label}
\end{figure}

In addition to these kind of visual tool, we also use the \textit{Kolmogorov-Smirnov test} (KS) and the $\chi^2$ \textit{test} to assess how similar/different the two distributions evaluated are. For the KS, we obtain : 
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{KS} & \textbf{p-values} & \textbf{statistics}  \\
        \hline
        MuonPt & 0.007 & 0.433\\
        invMass & 0.007 & 0.433\\
        MET\_pt & 0.239 & 0.267\\
        \hline
    \end{tabular}
    \caption{KS test between inputs and outputs for each observable.}
\end{table}
If the threshold is set at $5\%$ signifiance, only the MET\_pt generated distribution would be considered similar to the training sample corresponding observable.

and for $\chi^2$ :
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{$\chi^2$} & \textbf{p-values} & \textbf{statistics}  \\
        \hline
        MuonPt & 0 & 668.134\\
        invMass & 0.00002 & 71.200\\
        MET\_pt & 0.00003 & 70.594\\
        \hline
    \end{tabular}
     \caption{$\chi^2$ test between inputs and outputs for each observable.}
\end{table}

As seen in Fig.(\ref{all corr}), we can visualize the dependencies between the three selected variables for the generated data and the training sample. Although the zones for outputs are larger, the general behaviour of the dependencies remains similar.

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.42]{invMass_corr.png}
\end{figure}

\vspace{-\baselineskip} % Reduce space between figures

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.42]{MuonPt_corr.png}
\end{figure}

\vspace{-\baselineskip} % Reduce space between figures

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.42]{MET_pt_corr.png}
    \caption{Comparison of the input and output correlations between the different variables used.}
    \label{all corr}
\end{figure}
To get numerical values of the dependencies between observables, we can also compute the mutual information values for each pair of variables, the obtained values are :

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Mutual Information score} & \textbf{Outputs} & \textbf{Inputs} \\
        \hline
        MI(MuonPt, invMass) & 10.3075 & 8.6731 \\
        MI(invMass, MET\_pt) & 10.3080 & 8.6733 \\
        MI(MuonPt, MET\_pt) & 10.2908 & 8.6731 \\
        \hline
    \end{tabular}
    \caption{Mutual Information Values for a 3D cGAN.}
    \label{tab:mi_values}
\end{table}
These numbers might be hard to interpret, since the mutual information score ranges from 0 to $+\infty$. However, the exact values are not the important detail to remember, the similarity between the variables of the input and the output is. Indeed, in both cases, the MI score orbits around the same value, meaning that the correlations between observables is close to our goal, despite being slightly larger than expected.

\subsection{CMS data}

The final step is to apply the network to actual data, or not on a sample generated by MC simulation. The nTuple used contains data collected in 2022 by CMS, for approximately 13 000 events and with all leptons being muons.\\
Here are the variables specified in the nTuple :

\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Development/Figure 2024-05-18 173119 (2).png} % Replace with your image path
        
        \label{fig:figure1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Development/Figure 2024-05-18 173119 (0).png} % Replace with your image path
        
        \label{fig:figure2}
    \end{minipage}

    \vspace{1em} % Space between rows

    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Development/Figure 2024-05-18 173119 (1).png} % Replace with your image path
        
        \label{fig:figure3}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Development/Figure 2024-05-18 173119 (4).png} % Replace with your image path
        
        \label{fig:figure4}
    \end{minipage}
    \caption{Generated variables in the CMS nTuple.}
\end{figure}
